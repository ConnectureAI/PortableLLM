# PortableLLM Docker Compose Configuration
# Professional-Grade AI for Healthcare & Small Business
# Privacy-First • Local Processing • HIPAA Ready

version: '3.8'

services:
  # Main PortableLLM Application
  portablellm:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    image: portablellm/app:latest
    container_name: portablellm-app
    restart: unless-stopped
    
    ports:
      - "8080:8080"    # Web interface
      - "11434:11434"  # Ollama API
    
    volumes:
      # Persistent data storage
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
      - ./config:/app/config
      
      # Optional: Custom configuration
      - ./config/app.json:/app/config/app.json:ro
      
      # GPU access (if available)
      - /dev/nvidia0:/dev/nvidia0:rw
      - /dev/nvidiactl:/dev/nvidiactl:rw
      - /dev/nvidia-uvm:/dev/nvidia-uvm:rw
    
    environment:
      # Application Configuration
      - NODE_ENV=production
      - PORTABLELLM_MODE=healthcare
      - HOST=0.0.0.0
      - PORT=8080
      
      # Ollama Configuration
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_PORT=11434
      - OLLAMA_MODELS=/app/models
      - DEFAULT_MODEL=deepseek-coder:6.7b-instruct
      
      # Security & Privacy
      - LOCAL_ONLY=true
      - AUDIT_LOGGING=true
      - ENCRYPTION=true
      - HIPAA_MODE=true
      
      # Storage Paths
      - DATA_PATH=/app/data
      - LOGS_PATH=/app/logs
      - MODELS_PATH=/app/models
      - UPLOADS_PATH=/app/data/uploads
      
      # Logging
      - LOG_LEVEL=info
      - LOG_FORMAT=json
      
      # Performance
      - MAX_CONCURRENT_REQUESTS=5
      - REQUEST_TIMEOUT=300000
    
    # Resource limits for healthcare compliance
    deploy:
      resources:
        limits:
          memory: 16G
          cpus: '8.0'
        reservations:
          memory: 4G
          cpus: '2.0'
          # GPU support (uncomment if using NVIDIA GPU)
          # devices:
          #   - driver: nvidia
          #     count: 1
          #     capabilities: [gpu]
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    # Security settings
    security_opt:
      - no-new-privileges:true
    
    # User mapping for security
    user: "1001:1001"
    
    # Network isolation (healthcare compliance)
    networks:
      - portablellm-network
    
    # Logging configuration
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"
        labels: "service=portablellm"
    
    # Dependencies
    depends_on:
      - portablellm-proxy

  # Nginx reverse proxy for additional security
  portablellm-proxy:
    image: nginx:alpine
    container_name: portablellm-proxy
    restart: unless-stopped
    
    ports:
      - "80:80"
      - "443:443"
    
    volumes:
      - ./config/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./config/ssl:/etc/nginx/ssl:ro
      - ./logs/nginx:/var/log/nginx
    
    environment:
      - NGINX_HOST=localhost
      - NGINX_PORT=80
    
    networks:
      - portablellm-network
    
    # Security settings
    security_opt:
      - no-new-privileges:true
    
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Open WebUI (optional, for enhanced UI)
  portablellm-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: portablellm-webui
    restart: unless-stopped
    
    ports:
      - "3000:8080"
    
    volumes:
      - ./data/webui:/app/backend/data
    
    environment:
      - OLLAMA_BASE_URL=http://portablellm:11434
      - WEBUI_SECRET_KEY=your-secret-key
      - DEFAULT_MODELS=deepseek-coder:6.7b-instruct
      - ENABLE_SIGNUP=false
      - ENABLE_LOGIN_FORM=true
      - DEFAULT_USER_ROLE=user
    
    networks:
      - portablellm-network
    
    depends_on:
      - portablellm
    
    # Resource limits
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Database backup service
  portablellm-backup:
    image: alpine:latest
    container_name: portablellm-backup
    restart: unless-stopped
    
    volumes:
      - ./data:/backup/data:ro
      - ./backups:/backup/output
      - ./scripts/backup.sh:/backup/backup.sh:ro
    
    environment:
      - BACKUP_SCHEDULE=0 2 * * *  # Daily at 2 AM
      - BACKUP_RETENTION=7         # Keep 7 days
    
    command: >
      sh -c "
        apk add --no-cache dcron sqlite &&
        echo '0 2 * * * /backup/backup.sh' | crontab - &&
        crond -f
      "
    
    networks:
      - portablellm-network

# Network configuration
networks:
  portablellm-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
    driver_opts:
      com.docker.network.bridge.enable_ip_masquerade: "true"
      com.docker.network.bridge.enable_icc: "true"
      com.docker.network.driver.mtu: "1500"

# Named volumes for persistence
volumes:
  portablellm-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data
  
  portablellm-models:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./models
  
  portablellm-logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./logs

# Development override (use docker-compose.override.yml)
# Uncomment for development mode:
# services:
#   portablellm:
#     environment:
#       - NODE_ENV=development
#       - LOG_LEVEL=debug
#     volumes:
#       - ./src:/app/src
#     command: npm run dev